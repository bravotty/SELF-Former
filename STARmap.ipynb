{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import scipy.io as sio\n",
    "import torch.nn.functional as F\n",
    "from configure import get_default_config\n",
    "from model_starmap import Model\n",
    "from tools import normalize_type\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "dataset = {\n",
    "    0: \"dro\",\n",
    "    1: \"MERFISH\",\n",
    "    2: \"STARmap\",\n",
    "    4: \"BRCA\",\n",
    "    5: \"ELSE\",\n",
    "}\n",
    "data_name = dataset[2]\n",
    "shuffle = False\n",
    "random.seed(48)\n",
    "ktimes = 10\n",
    "cv = KFold(n_splits=ktimes, shuffle=False)\n",
    "mat_name = 'data/' + data_name + '.mat'\n",
    "data = sio.loadmat(mat_name)\n",
    "genes = [i for item in data['genes'].flatten() for i in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pearsonr(x, y):\n",
    "    mean_x = torch.mean(x)\n",
    "    mean_y = torch.mean(y)\n",
    "    xm = x.sub(mean_x)\n",
    "    ym = y.sub(mean_y)\n",
    "    r_num = xm.dot(ym)\n",
    "    r_den = torch.norm(xm, 2) * torch.norm(ym, 2)\n",
    "    r_val = r_num / (r_den + 1e-8)\n",
    "    r_val = torch.nan_to_num(r_val,nan=-1)\n",
    "    return r_val\n",
    "\n",
    "def correlationMetric(x, y):\n",
    "    corr = 0\n",
    "    for idx in range(x.size(1)):\n",
    "        corr += pearsonr(x[:,idx], y[:,idx])\n",
    "    corr /= (idx + 1)\n",
    "    return (1 - corr).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MinMaxScaler_torch:\n",
    "    def __init__(self, feature_range=(0, 1)):\n",
    "        self.min_val = None\n",
    "        self.max_val = None\n",
    "        self.feature_range = feature_range\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.min_val, _ = torch.min(data, dim=0)\n",
    "        self.max_val, _ = torch.max(data, dim=0)\n",
    "\n",
    "    def transform(self, data):\n",
    "        data_minmax = (data - self.min_val) / (self.max_val - self.min_val)\n",
    "        data_minmax = data_minmax * (self.feature_range[1] - self.feature_range[0]) + self.feature_range[0]\n",
    "        return data_minmax\n",
    "\n",
    "    def inverse_transform(self, data_minmax):\n",
    "        data = (data_minmax - self.feature_range[0]) / (self.feature_range[1] - self.feature_range[0])\n",
    "        data = data * (self.max_val - self.min_val) + self.min_val\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, x1, x2, config, model_path):\n",
    "    print('\\n===========> Training... <===========')\n",
    "    model.train()\n",
    "    if not isinstance(x1, torch.Tensor):\n",
    "        x1 = torch.from_numpy(x1).to(device)\n",
    "    if not isinstance(x2, torch.Tensor):\n",
    "        x2 = torch.from_numpy(x2).to(device)\n",
    "    for epoch in range(config['pretrain_epochs']):\n",
    "        x1_hat = model(x2)\n",
    "        mask_zero = x1 != 0\n",
    "        x1_masked = torch.masked_select(x1, mask_zero)\n",
    "        x1_masked_pred = torch.masked_select(x1_hat, mask_zero)\n",
    "        loss_ae = F.mse_loss(x1, x1_hat, reduction='mean')\n",
    "        loss_mask_ae = F.mse_loss(x1_masked, x1_masked_pred, reduction='mean')\n",
    "        corrloss = correlationMetric(x1_hat, x1)\n",
    "        loss = loss_ae + 0.001 * loss_mask_ae + 0.01 * corrloss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print ('train epoch %d: loss_ae: %.6f loss_mask_ae: %.6f loss_corr: %.6f' \\\n",
    "                   % (epoch, loss.item(), loss_mask_ae.item(), corrloss.item()))\n",
    "    torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_loc = data['locations']\n",
    "atlas_genes = data['genes']\n",
    "atlas_genes = atlas_genes.reshape(-1)\n",
    "atlas_genes = [i for item in atlas_genes for i in item]\n",
    "x1_cell = data['ST']\n",
    "x2_rna = data['rna']\n",
    "cnts = 0\n",
    "tmp_dims = 0\n",
    "result = np.empty((x1_cell.shape[0], 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1549, 2) (1549, 996) (15413, 996)\n",
      "333.0 0.0 411165.0 0.0\n",
      "Before Norm ST: 0.7901958636908605 Before Norm RNA: 0.5123166841997943\n",
      "Xrna shape: (15413, 896) Xst shape: (1549, 896)\n",
      "ST: 0.7901965842017892 RNA: 0.5123166841997943\n",
      "1.0 0.0 0.05263017 1.0 0.0 0.036019653\n",
      "train/STARmap_0.pkl\n",
      "{'dims': [2222, 512, 256], 'pretrain_epochs': 400, 'epochs': 15, 'pre_lr': 0.0001, 'lr': 0.0001, 'batch_size': 256, 'weight_map': 10, 'weight_coef': 0, 'weight_mmd': 0, 'weight_ent': 0, 'alpha': 1, 'ot': {'epochs': 300, 'lr': 0.05, 'step_size': 300, 'tau': 1, 'it': 3, 'epsilon': 1, 'num_iter': 5}, 'num_sample1': 1549, 'num_sample2': 15413}\n",
      "\n",
      "===========> Training... <===========\n",
      "train epoch 0: loss_ae: 0.028833 loss_mask_ae: 0.087751 loss_corr: 1.005404\n",
      "train epoch 10: loss_ae: 0.023509 loss_mask_ae: 0.064130 loss_corr: 0.804869\n",
      "train epoch 20: loss_ae: 0.022450 loss_mask_ae: 0.056524 loss_corr: 0.772336\n",
      "train epoch 30: loss_ae: 0.022311 loss_mask_ae: 0.057325 loss_corr: 0.766850\n",
      "train epoch 40: loss_ae: 0.022258 loss_mask_ae: 0.057390 loss_corr: 0.765005\n",
      "train epoch 50: loss_ae: 0.022224 loss_mask_ae: 0.056820 loss_corr: 0.764219\n",
      "train epoch 60: loss_ae: 0.022189 loss_mask_ae: 0.057067 loss_corr: 0.763462\n",
      "train epoch 70: loss_ae: 0.022119 loss_mask_ae: 0.056438 loss_corr: 0.762130\n",
      "train epoch 80: loss_ae: 0.022117 loss_mask_ae: 0.059504 loss_corr: 0.761333\n",
      "train epoch 90: loss_ae: 0.021941 loss_mask_ae: 0.057559 loss_corr: 0.758676\n",
      "train epoch 100: loss_ae: 0.021820 loss_mask_ae: 0.053531 loss_corr: 0.757853\n",
      "train epoch 110: loss_ae: 0.021724 loss_mask_ae: 0.056706 loss_corr: 0.760737\n",
      "train epoch 120: loss_ae: 0.021623 loss_mask_ae: 0.056936 loss_corr: 0.756574\n",
      "train epoch 130: loss_ae: 0.021503 loss_mask_ae: 0.052991 loss_corr: 0.753942\n",
      "train epoch 140: loss_ae: 0.021378 loss_mask_ae: 0.053235 loss_corr: 0.750825\n",
      "train epoch 150: loss_ae: 0.021455 loss_mask_ae: 0.058038 loss_corr: 0.748473\n",
      "train epoch 160: loss_ae: 0.021292 loss_mask_ae: 0.050047 loss_corr: 0.743444\n",
      "train epoch 170: loss_ae: 0.021047 loss_mask_ae: 0.052963 loss_corr: 0.739487\n",
      "train epoch 180: loss_ae: 0.020982 loss_mask_ae: 0.054244 loss_corr: 0.736639\n",
      "train epoch 190: loss_ae: 0.020895 loss_mask_ae: 0.052085 loss_corr: 0.733557\n",
      "train epoch 200: loss_ae: 0.020945 loss_mask_ae: 0.051910 loss_corr: 0.736114\n",
      "train epoch 210: loss_ae: 0.020830 loss_mask_ae: 0.053769 loss_corr: 0.730163\n",
      "train epoch 220: loss_ae: 0.020770 loss_mask_ae: 0.053108 loss_corr: 0.727939\n",
      "train epoch 230: loss_ae: 0.020713 loss_mask_ae: 0.052184 loss_corr: 0.725605\n",
      "train epoch 240: loss_ae: 0.020670 loss_mask_ae: 0.052496 loss_corr: 0.723398\n",
      "train epoch 250: loss_ae: 0.020663 loss_mask_ae: 0.051974 loss_corr: 0.722844\n",
      "train epoch 260: loss_ae: 0.020600 loss_mask_ae: 0.052017 loss_corr: 0.719730\n",
      "train epoch 270: loss_ae: 0.020559 loss_mask_ae: 0.052127 loss_corr: 0.717441\n",
      "train epoch 280: loss_ae: 0.020526 loss_mask_ae: 0.050473 loss_corr: 0.715221\n",
      "train epoch 290: loss_ae: 0.020458 loss_mask_ae: 0.052277 loss_corr: 0.711839\n",
      "train epoch 300: loss_ae: 0.020416 loss_mask_ae: 0.052273 loss_corr: 0.709369\n",
      "train epoch 310: loss_ae: 0.020374 loss_mask_ae: 0.052283 loss_corr: 0.706848\n",
      "train epoch 320: loss_ae: 0.020326 loss_mask_ae: 0.052244 loss_corr: 0.703971\n",
      "train epoch 330: loss_ae: 0.020293 loss_mask_ae: 0.052122 loss_corr: 0.701958\n",
      "train epoch 340: loss_ae: 0.020310 loss_mask_ae: 0.050756 loss_corr: 0.702790\n",
      "train epoch 350: loss_ae: 0.020210 loss_mask_ae: 0.049767 loss_corr: 0.697349\n",
      "train epoch 360: loss_ae: 0.020171 loss_mask_ae: 0.051418 loss_corr: 0.695505\n",
      "train epoch 370: loss_ae: 0.020107 loss_mask_ae: 0.050241 loss_corr: 0.692260\n",
      "train epoch 380: loss_ae: 0.020114 loss_mask_ae: 0.050474 loss_corr: 0.692439\n",
      "train epoch 390: loss_ae: 0.020061 loss_mask_ae: 0.050475 loss_corr: 0.689656\n",
      "Finished 0 Times Training and Testing...\n",
      "(1549, 2) (1549, 996) (15413, 996)\n",
      "333.0 0.0 411165.0 0.0\n",
      "Before Norm ST: 0.7868822339297242 Before Norm RNA: 0.5003501074000611\n",
      "Xrna shape: (15413, 896) Xst shape: (1549, 896)\n",
      "ST: 0.7868829544406529 RNA: 0.5003501074000611\n",
      "1.0 0.0 0.05334913 1.0 0.0 0.037379134\n",
      "train/STARmap_1.pkl\n",
      "{'dims': [2222, 512, 256], 'pretrain_epochs': 400, 'epochs': 15, 'pre_lr': 0.0001, 'lr': 0.0001, 'batch_size': 256, 'weight_map': 10, 'weight_coef': 0, 'weight_mmd': 0, 'weight_ent': 0, 'alpha': 1, 'ot': {'epochs': 300, 'lr': 0.05, 'step_size': 300, 'tau': 1, 'it': 3, 'epsilon': 1, 'num_iter': 5}, 'num_sample1': 1549, 'num_sample2': 15413}\n",
      "\n",
      "===========> Training... <===========\n",
      "train epoch 0: loss_ae: 0.028912 loss_mask_ae: 0.087030 loss_corr: 1.000297\n",
      "train epoch 10: loss_ae: 0.023613 loss_mask_ae: 0.064370 loss_corr: 0.802273\n",
      "train epoch 20: loss_ae: 0.022564 loss_mask_ae: 0.055582 loss_corr: 0.769414\n",
      "train epoch 30: loss_ae: 0.022424 loss_mask_ae: 0.057360 loss_corr: 0.764093\n",
      "train epoch 40: loss_ae: 0.022370 loss_mask_ae: 0.056707 loss_corr: 0.762295\n",
      "train epoch 50: loss_ae: 0.022337 loss_mask_ae: 0.056412 loss_corr: 0.761435\n",
      "train epoch 60: loss_ae: 0.022304 loss_mask_ae: 0.056596 loss_corr: 0.760726\n",
      "train epoch 70: loss_ae: 0.022257 loss_mask_ae: 0.056142 loss_corr: 0.759394\n",
      "train epoch 80: loss_ae: 0.022169 loss_mask_ae: 0.055768 loss_corr: 0.757438\n",
      "train epoch 90: loss_ae: 0.022060 loss_mask_ae: 0.054031 loss_corr: 0.755606\n",
      "train epoch 100: loss_ae: 0.021930 loss_mask_ae: 0.053905 loss_corr: 0.753968\n",
      "train epoch 110: loss_ae: 0.021787 loss_mask_ae: 0.053283 loss_corr: 0.752936\n",
      "train epoch 120: loss_ae: 0.022315 loss_mask_ae: 0.048453 loss_corr: 0.759356\n",
      "train epoch 130: loss_ae: 0.021736 loss_mask_ae: 0.054460 loss_corr: 0.753753\n",
      "train epoch 140: loss_ae: 0.021626 loss_mask_ae: 0.055908 loss_corr: 0.750714\n",
      "train epoch 150: loss_ae: 0.021468 loss_mask_ae: 0.054516 loss_corr: 0.746510\n",
      "train epoch 160: loss_ae: 0.021320 loss_mask_ae: 0.053058 loss_corr: 0.741845\n",
      "train epoch 170: loss_ae: 0.021194 loss_mask_ae: 0.053059 loss_corr: 0.737994\n",
      "train epoch 180: loss_ae: 0.021087 loss_mask_ae: 0.052688 loss_corr: 0.734724\n",
      "train epoch 190: loss_ae: 0.021009 loss_mask_ae: 0.051749 loss_corr: 0.731901\n",
      "train epoch 200: loss_ae: 0.020927 loss_mask_ae: 0.052182 loss_corr: 0.729088\n",
      "train epoch 210: loss_ae: 0.020930 loss_mask_ae: 0.052857 loss_corr: 0.728838\n",
      "train epoch 220: loss_ae: 0.020856 loss_mask_ae: 0.051123 loss_corr: 0.725424\n",
      "train epoch 230: loss_ae: 0.020803 loss_mask_ae: 0.051038 loss_corr: 0.723352\n",
      "train epoch 240: loss_ae: 0.020756 loss_mask_ae: 0.051049 loss_corr: 0.721073\n",
      "train epoch 250: loss_ae: 0.020715 loss_mask_ae: 0.051462 loss_corr: 0.719065\n",
      "train epoch 260: loss_ae: 0.020766 loss_mask_ae: 0.052650 loss_corr: 0.720425\n",
      "train epoch 270: loss_ae: 0.020700 loss_mask_ae: 0.051776 loss_corr: 0.717514\n",
      "train epoch 280: loss_ae: 0.020610 loss_mask_ae: 0.051399 loss_corr: 0.712983\n",
      "train epoch 290: loss_ae: 0.020560 loss_mask_ae: 0.051241 loss_corr: 0.710275\n",
      "train epoch 300: loss_ae: 0.020512 loss_mask_ae: 0.050537 loss_corr: 0.707476\n",
      "train epoch 310: loss_ae: 0.020562 loss_mask_ae: 0.049254 loss_corr: 0.708474\n",
      "train epoch 320: loss_ae: 0.020439 loss_mask_ae: 0.051013 loss_corr: 0.703533\n",
      "train epoch 330: loss_ae: 0.020405 loss_mask_ae: 0.051216 loss_corr: 0.701680\n",
      "train epoch 340: loss_ae: 0.020382 loss_mask_ae: 0.049904 loss_corr: 0.699876\n",
      "train epoch 350: loss_ae: 0.020340 loss_mask_ae: 0.050155 loss_corr: 0.698308\n",
      "train epoch 360: loss_ae: 0.020288 loss_mask_ae: 0.050648 loss_corr: 0.695878\n",
      "train epoch 370: loss_ae: 0.020269 loss_mask_ae: 0.049025 loss_corr: 0.693908\n",
      "train epoch 380: loss_ae: 0.020235 loss_mask_ae: 0.049893 loss_corr: 0.692847\n",
      "train epoch 390: loss_ae: 0.020203 loss_mask_ae: 0.051876 loss_corr: 0.690207\n",
      "Finished 1 Times Training and Testing...\n",
      "(1549, 2) (1549, 996) (15413, 996)\n",
      "333.0 0.0 411165.0 0.0\n",
      "Before Norm ST: 0.7898031852347136 Before Norm RNA: 0.5151208018972853\n",
      "Xrna shape: (15413, 896) Xst shape: (1549, 896)\n",
      "ST: 0.7898039057456423 RNA: 0.5151208018972853\n",
      "1.0 0.0 0.052609384 1.0 0.0 0.035778504\n",
      "train/STARmap_2.pkl\n",
      "{'dims': [2222, 512, 256], 'pretrain_epochs': 400, 'epochs': 15, 'pre_lr': 0.0001, 'lr': 0.0001, 'batch_size': 256, 'weight_map': 10, 'weight_coef': 0, 'weight_mmd': 0, 'weight_ent': 0, 'alpha': 1, 'ot': {'epochs': 300, 'lr': 0.05, 'step_size': 300, 'tau': 1, 'it': 3, 'epsilon': 1, 'num_iter': 5}, 'num_sample1': 1549, 'num_sample2': 15413}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========> Training... <===========\n",
      "train epoch 0: loss_ae: 0.028779 loss_mask_ae: 0.087262 loss_corr: 1.006526\n",
      "train epoch 10: loss_ae: 0.023559 loss_mask_ae: 0.064189 loss_corr: 0.806980\n",
      "train epoch 20: loss_ae: 0.022475 loss_mask_ae: 0.056684 loss_corr: 0.771737\n",
      "train epoch 30: loss_ae: 0.022351 loss_mask_ae: 0.057663 loss_corr: 0.767149\n",
      "train epoch 40: loss_ae: 0.022298 loss_mask_ae: 0.057733 loss_corr: 0.765096\n",
      "train epoch 50: loss_ae: 0.022266 loss_mask_ae: 0.057373 loss_corr: 0.764223\n",
      "train epoch 60: loss_ae: 0.022236 loss_mask_ae: 0.056924 loss_corr: 0.763687\n",
      "train epoch 70: loss_ae: 0.022190 loss_mask_ae: 0.056734 loss_corr: 0.762828\n",
      "train epoch 80: loss_ae: 0.022199 loss_mask_ae: 0.059591 loss_corr: 0.762820\n",
      "train epoch 90: loss_ae: 0.022069 loss_mask_ae: 0.054668 loss_corr: 0.760804\n",
      "train epoch 100: loss_ae: 0.021974 loss_mask_ae: 0.056222 loss_corr: 0.758667\n",
      "train epoch 110: loss_ae: 0.021870 loss_mask_ae: 0.055415 loss_corr: 0.757757\n",
      "train epoch 120: loss_ae: 0.021711 loss_mask_ae: 0.055165 loss_corr: 0.757339\n",
      "train epoch 130: loss_ae: 0.022350 loss_mask_ae: 0.054887 loss_corr: 0.766343\n",
      "train epoch 140: loss_ae: 0.021906 loss_mask_ae: 0.056611 loss_corr: 0.759977\n",
      "train epoch 150: loss_ae: 0.021680 loss_mask_ae: 0.054812 loss_corr: 0.756492\n",
      "train epoch 160: loss_ae: 0.021547 loss_mask_ae: 0.054596 loss_corr: 0.753645\n",
      "train epoch 170: loss_ae: 0.021425 loss_mask_ae: 0.054541 loss_corr: 0.750142\n",
      "train epoch 180: loss_ae: 0.021285 loss_mask_ae: 0.053962 loss_corr: 0.745745\n",
      "train epoch 190: loss_ae: 0.021630 loss_mask_ae: 0.059894 loss_corr: 0.747939\n",
      "train epoch 200: loss_ae: 0.021405 loss_mask_ae: 0.057315 loss_corr: 0.744833\n",
      "train epoch 210: loss_ae: 0.021189 loss_mask_ae: 0.050271 loss_corr: 0.739207\n",
      "train epoch 220: loss_ae: 0.021025 loss_mask_ae: 0.052955 loss_corr: 0.736238\n",
      "train epoch 230: loss_ae: 0.020966 loss_mask_ae: 0.053490 loss_corr: 0.733976\n",
      "train epoch 240: loss_ae: 0.020901 loss_mask_ae: 0.053084 loss_corr: 0.731325\n",
      "train epoch 250: loss_ae: 0.020835 loss_mask_ae: 0.052542 loss_corr: 0.728293\n",
      "train epoch 260: loss_ae: 0.020784 loss_mask_ae: 0.052322 loss_corr: 0.725577\n",
      "train epoch 270: loss_ae: 0.020764 loss_mask_ae: 0.052718 loss_corr: 0.724385\n",
      "train epoch 280: loss_ae: 0.020675 loss_mask_ae: 0.052381 loss_corr: 0.720252\n",
      "train epoch 290: loss_ae: 0.020620 loss_mask_ae: 0.052190 loss_corr: 0.717702\n",
      "train epoch 300: loss_ae: 0.020570 loss_mask_ae: 0.052085 loss_corr: 0.715236\n",
      "train epoch 310: loss_ae: 0.020520 loss_mask_ae: 0.051877 loss_corr: 0.712889\n",
      "train epoch 320: loss_ae: 0.020512 loss_mask_ae: 0.051943 loss_corr: 0.712394\n",
      "train epoch 330: loss_ae: 0.020445 loss_mask_ae: 0.051792 loss_corr: 0.709179\n",
      "train epoch 340: loss_ae: 0.020419 loss_mask_ae: 0.051760 loss_corr: 0.707860\n",
      "train epoch 350: loss_ae: 0.020390 loss_mask_ae: 0.051445 loss_corr: 0.706257\n",
      "train epoch 360: loss_ae: 0.020347 loss_mask_ae: 0.052100 loss_corr: 0.704052\n",
      "train epoch 370: loss_ae: 0.020325 loss_mask_ae: 0.050591 loss_corr: 0.702554\n",
      "train epoch 380: loss_ae: 0.020356 loss_mask_ae: 0.052361 loss_corr: 0.703739\n",
      "train epoch 390: loss_ae: 0.020266 loss_mask_ae: 0.050148 loss_corr: 0.699271\n",
      "Finished 2 Times Training and Testing...\n",
      "(1549, 2) (1549, 996) (15413, 996)\n",
      "333.0 0.0 411165.0 0.0\n",
      "Before Norm ST: 0.7914416270865996 Before Norm RNA: 0.5133050949569473\n",
      "Xrna shape: (15413, 896) Xst shape: (1549, 896)\n",
      "ST: 0.7914423475975284 RNA: 0.5133050949569473\n",
      "1.0 0.0 0.05228175 1.0 0.0 0.035992954\n",
      "train/STARmap_3.pkl\n",
      "{'dims': [2222, 512, 256], 'pretrain_epochs': 400, 'epochs': 15, 'pre_lr': 0.0001, 'lr': 0.0001, 'batch_size': 256, 'weight_map': 10, 'weight_coef': 0, 'weight_mmd': 0, 'weight_ent': 0, 'alpha': 1, 'ot': {'epochs': 300, 'lr': 0.05, 'step_size': 300, 'tau': 1, 'it': 3, 'epsilon': 1, 'num_iter': 5}, 'num_sample1': 1549, 'num_sample2': 15413}\n",
      "\n",
      "===========> Training... <===========\n",
      "train epoch 0: loss_ae: 0.028675 loss_mask_ae: 0.087619 loss_corr: 1.001138\n",
      "train epoch 10: loss_ae: 0.023497 loss_mask_ae: 0.065471 loss_corr: 0.805858\n",
      "train epoch 20: loss_ae: 0.022404 loss_mask_ae: 0.057007 loss_corr: 0.770994\n",
      "train epoch 30: loss_ae: 0.022270 loss_mask_ae: 0.057608 loss_corr: 0.765293\n",
      "train epoch 40: loss_ae: 0.022225 loss_mask_ae: 0.057931 loss_corr: 0.763563\n",
      "train epoch 50: loss_ae: 0.022190 loss_mask_ae: 0.057452 loss_corr: 0.762753\n",
      "train epoch 60: loss_ae: 0.022157 loss_mask_ae: 0.057203 loss_corr: 0.761903\n",
      "train epoch 70: loss_ae: 0.022101 loss_mask_ae: 0.056981 loss_corr: 0.760472\n",
      "train epoch 80: loss_ae: 0.022059 loss_mask_ae: 0.058997 loss_corr: 0.759329\n",
      "train epoch 90: loss_ae: 0.021966 loss_mask_ae: 0.058709 loss_corr: 0.757569\n",
      "train epoch 100: loss_ae: 0.021787 loss_mask_ae: 0.054626 loss_corr: 0.755776\n",
      "train epoch 110: loss_ae: 0.021731 loss_mask_ae: 0.057806 loss_corr: 0.756304\n",
      "train epoch 120: loss_ae: 0.021558 loss_mask_ae: 0.053949 loss_corr: 0.754415\n",
      "train epoch 130: loss_ae: 0.021434 loss_mask_ae: 0.054914 loss_corr: 0.752125\n",
      "train epoch 140: loss_ae: 0.021299 loss_mask_ae: 0.054650 loss_corr: 0.748037\n",
      "train epoch 150: loss_ae: 0.021183 loss_mask_ae: 0.052235 loss_corr: 0.743188\n",
      "train epoch 160: loss_ae: 0.021172 loss_mask_ae: 0.050311 loss_corr: 0.739633\n",
      "train epoch 170: loss_ae: 0.021035 loss_mask_ae: 0.055257 loss_corr: 0.737133\n",
      "train epoch 180: loss_ae: 0.020934 loss_mask_ae: 0.052034 loss_corr: 0.735281\n",
      "train epoch 190: loss_ae: 0.020847 loss_mask_ae: 0.052943 loss_corr: 0.732515\n",
      "train epoch 200: loss_ae: 0.020950 loss_mask_ae: 0.054792 loss_corr: 0.735644\n",
      "train epoch 210: loss_ae: 0.020791 loss_mask_ae: 0.051426 loss_corr: 0.729403\n",
      "train epoch 220: loss_ae: 0.020727 loss_mask_ae: 0.052013 loss_corr: 0.727078\n",
      "train epoch 230: loss_ae: 0.020679 loss_mask_ae: 0.052651 loss_corr: 0.724901\n",
      "train epoch 240: loss_ae: 0.020641 loss_mask_ae: 0.052456 loss_corr: 0.722966\n",
      "train epoch 250: loss_ae: 0.020643 loss_mask_ae: 0.053006 loss_corr: 0.722647\n",
      "train epoch 260: loss_ae: 0.020583 loss_mask_ae: 0.052887 loss_corr: 0.719759\n",
      "train epoch 270: loss_ae: 0.020536 loss_mask_ae: 0.051591 loss_corr: 0.717144\n",
      "train epoch 280: loss_ae: 0.020501 loss_mask_ae: 0.052303 loss_corr: 0.715348\n",
      "train epoch 290: loss_ae: 0.020483 loss_mask_ae: 0.053089 loss_corr: 0.713760\n",
      "train epoch 300: loss_ae: 0.020414 loss_mask_ae: 0.051730 loss_corr: 0.710368\n",
      "train epoch 310: loss_ae: 0.020421 loss_mask_ae: 0.052540 loss_corr: 0.709940\n",
      "train epoch 320: loss_ae: 0.020353 loss_mask_ae: 0.051857 loss_corr: 0.706753\n",
      "train epoch 330: loss_ae: 0.020297 loss_mask_ae: 0.050999 loss_corr: 0.703736\n",
      "train epoch 340: loss_ae: 0.020279 loss_mask_ae: 0.050744 loss_corr: 0.702368\n",
      "train epoch 350: loss_ae: 0.020228 loss_mask_ae: 0.052417 loss_corr: 0.699754\n",
      "train epoch 360: loss_ae: 0.020199 loss_mask_ae: 0.051002 loss_corr: 0.698761\n",
      "train epoch 370: loss_ae: 0.020205 loss_mask_ae: 0.050037 loss_corr: 0.697865\n",
      "train epoch 380: loss_ae: 0.020131 loss_mask_ae: 0.050908 loss_corr: 0.694901\n",
      "train epoch 390: loss_ae: 0.020068 loss_mask_ae: 0.051456 loss_corr: 0.691771\n",
      "Finished 3 Times Training and Testing...\n",
      "(1549, 2) (1549, 996) (15413, 996)\n",
      "333.0 0.0 411165.0 0.0\n",
      "Before Norm ST: 0.7893550274370562 Before Norm RNA: 0.5075119217543632\n",
      "Xrna shape: (15413, 896) Xst shape: (1549, 896)\n",
      "ST: 0.7893557479479849 RNA: 0.5075119217543632\n",
      "1.0 0.0 0.052571777 1.0 0.0 0.03662849\n",
      "train/STARmap_4.pkl\n",
      "{'dims': [2222, 512, 256], 'pretrain_epochs': 400, 'epochs': 15, 'pre_lr': 0.0001, 'lr': 0.0001, 'batch_size': 256, 'weight_map': 10, 'weight_coef': 0, 'weight_mmd': 0, 'weight_ent': 0, 'alpha': 1, 'ot': {'epochs': 300, 'lr': 0.05, 'step_size': 300, 'tau': 1, 'it': 3, 'epsilon': 1, 'num_iter': 5}, 'num_sample1': 1549, 'num_sample2': 15413}\n",
      "\n",
      "===========> Training... <===========\n",
      "train epoch 0: loss_ae: 0.028759 loss_mask_ae: 0.087231 loss_corr: 1.001092\n",
      "train epoch 10: loss_ae: 0.023474 loss_mask_ae: 0.063251 loss_corr: 0.802834\n",
      "train epoch 20: loss_ae: 0.022360 loss_mask_ae: 0.057410 loss_corr: 0.769133\n",
      "train epoch 30: loss_ae: 0.022252 loss_mask_ae: 0.057193 loss_corr: 0.764199\n",
      "train epoch 40: loss_ae: 0.022198 loss_mask_ae: 0.057208 loss_corr: 0.762362\n",
      "train epoch 50: loss_ae: 0.022166 loss_mask_ae: 0.056504 loss_corr: 0.761618\n",
      "train epoch 60: loss_ae: 0.022135 loss_mask_ae: 0.056615 loss_corr: 0.760984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 70: loss_ae: 0.022080 loss_mask_ae: 0.056394 loss_corr: 0.760123\n",
      "train epoch 80: loss_ae: 0.022097 loss_mask_ae: 0.052654 loss_corr: 0.759855\n",
      "train epoch 90: loss_ae: 0.021944 loss_mask_ae: 0.057702 loss_corr: 0.757686\n",
      "train epoch 100: loss_ae: 0.021775 loss_mask_ae: 0.056147 loss_corr: 0.755802\n",
      "train epoch 110: loss_ae: 0.021673 loss_mask_ae: 0.052661 loss_corr: 0.755059\n",
      "train epoch 120: loss_ae: 0.021735 loss_mask_ae: 0.056508 loss_corr: 0.755290\n",
      "train epoch 130: loss_ae: 0.021629 loss_mask_ae: 0.051372 loss_corr: 0.754022\n",
      "train epoch 140: loss_ae: 0.021434 loss_mask_ae: 0.054499 loss_corr: 0.751108\n",
      "train epoch 150: loss_ae: 0.021328 loss_mask_ae: 0.054307 loss_corr: 0.747942\n",
      "train epoch 160: loss_ae: 0.021182 loss_mask_ae: 0.053997 loss_corr: 0.743024\n",
      "train epoch 170: loss_ae: 0.021040 loss_mask_ae: 0.053448 loss_corr: 0.738374\n",
      "train epoch 180: loss_ae: 0.021012 loss_mask_ae: 0.051124 loss_corr: 0.735925\n",
      "train epoch 190: loss_ae: 0.020905 loss_mask_ae: 0.052284 loss_corr: 0.733594\n",
      "train epoch 200: loss_ae: 0.020897 loss_mask_ae: 0.050890 loss_corr: 0.732207\n",
      "train epoch 210: loss_ae: 0.020808 loss_mask_ae: 0.051037 loss_corr: 0.728801\n",
      "train epoch 220: loss_ae: 0.020805 loss_mask_ae: 0.052887 loss_corr: 0.729487\n",
      "train epoch 230: loss_ae: 0.020717 loss_mask_ae: 0.052396 loss_corr: 0.725712\n",
      "train epoch 240: loss_ae: 0.020674 loss_mask_ae: 0.052316 loss_corr: 0.723764\n",
      "train epoch 250: loss_ae: 0.020632 loss_mask_ae: 0.051766 loss_corr: 0.721454\n",
      "train epoch 260: loss_ae: 0.020590 loss_mask_ae: 0.051998 loss_corr: 0.719342\n",
      "train epoch 270: loss_ae: 0.020557 loss_mask_ae: 0.052135 loss_corr: 0.717424\n",
      "train epoch 280: loss_ae: 0.020533 loss_mask_ae: 0.051368 loss_corr: 0.715931\n",
      "train epoch 290: loss_ae: 0.020486 loss_mask_ae: 0.051368 loss_corr: 0.713274\n",
      "train epoch 300: loss_ae: 0.020447 loss_mask_ae: 0.052032 loss_corr: 0.710915\n",
      "train epoch 310: loss_ae: 0.020406 loss_mask_ae: 0.051571 loss_corr: 0.708481\n",
      "train epoch 320: loss_ae: 0.020334 loss_mask_ae: 0.050613 loss_corr: 0.704720\n",
      "train epoch 330: loss_ae: 0.020309 loss_mask_ae: 0.051506 loss_corr: 0.703377\n",
      "train epoch 340: loss_ae: 0.020243 loss_mask_ae: 0.051443 loss_corr: 0.699822\n",
      "train epoch 350: loss_ae: 0.020223 loss_mask_ae: 0.051506 loss_corr: 0.698610\n",
      "train epoch 360: loss_ae: 0.020149 loss_mask_ae: 0.050069 loss_corr: 0.694827\n",
      "train epoch 370: loss_ae: 0.020290 loss_mask_ae: 0.050800 loss_corr: 0.702717\n",
      "train epoch 380: loss_ae: 0.020125 loss_mask_ae: 0.050422 loss_corr: 0.693714\n",
      "train epoch 390: loss_ae: 0.020066 loss_mask_ae: 0.050118 loss_corr: 0.690255\n",
      "Finished 4 Times Training and Testing...\n",
      "(1549, 2) (1549, 996) (15413, 996)\n",
      "333.0 0.0 411165.0 0.0\n",
      "Before Norm ST: 0.788700083002859 Before Norm RNA: 0.5104231353866402\n",
      "Xrna shape: (15413, 896) Xst shape: (1549, 896)\n",
      "ST: 0.7887008035137877 RNA: 0.5104231353866402\n",
      "1.0 0.0 0.05263176 1.0 0.0 0.035961885\n",
      "train/STARmap_5.pkl\n",
      "{'dims': [2222, 512, 256], 'pretrain_epochs': 400, 'epochs': 15, 'pre_lr': 0.0001, 'lr': 0.0001, 'batch_size': 256, 'weight_map': 10, 'weight_coef': 0, 'weight_mmd': 0, 'weight_ent': 0, 'alpha': 1, 'ot': {'epochs': 300, 'lr': 0.05, 'step_size': 300, 'tau': 1, 'it': 3, 'epsilon': 1, 'num_iter': 5}, 'num_sample1': 1549, 'num_sample2': 15413}\n",
      "\n",
      "===========> Training... <===========\n",
      "train epoch 0: loss_ae: 0.028698 loss_mask_ae: 0.086663 loss_corr: 1.000375\n",
      "train epoch 10: loss_ae: 0.023461 loss_mask_ae: 0.065078 loss_corr: 0.803023\n",
      "train epoch 20: loss_ae: 0.022352 loss_mask_ae: 0.055247 loss_corr: 0.769542\n",
      "train epoch 30: loss_ae: 0.022215 loss_mask_ae: 0.057124 loss_corr: 0.764395\n",
      "train epoch 40: loss_ae: 0.022160 loss_mask_ae: 0.056540 loss_corr: 0.762481\n",
      "train epoch 50: loss_ae: 0.022131 loss_mask_ae: 0.056237 loss_corr: 0.761655\n",
      "train epoch 60: loss_ae: 0.022104 loss_mask_ae: 0.056405 loss_corr: 0.761055\n",
      "train epoch 70: loss_ae: 0.022064 loss_mask_ae: 0.055947 loss_corr: 0.760253\n",
      "train epoch 80: loss_ae: 0.021982 loss_mask_ae: 0.055155 loss_corr: 0.759151\n",
      "train epoch 90: loss_ae: 0.021948 loss_mask_ae: 0.057165 loss_corr: 0.758468\n",
      "train epoch 100: loss_ae: 0.021777 loss_mask_ae: 0.054364 loss_corr: 0.756328\n",
      "train epoch 110: loss_ae: 0.021663 loss_mask_ae: 0.055681 loss_corr: 0.755724\n",
      "train epoch 120: loss_ae: 0.021544 loss_mask_ae: 0.054281 loss_corr: 0.756146\n",
      "train epoch 130: loss_ae: 0.021451 loss_mask_ae: 0.053896 loss_corr: 0.753617\n",
      "train epoch 140: loss_ae: 0.021420 loss_mask_ae: 0.052435 loss_corr: 0.752199\n",
      "train epoch 150: loss_ae: 0.021292 loss_mask_ae: 0.053486 loss_corr: 0.750422\n",
      "train epoch 160: loss_ae: 0.021230 loss_mask_ae: 0.052460 loss_corr: 0.748672\n",
      "train epoch 170: loss_ae: 0.021163 loss_mask_ae: 0.051762 loss_corr: 0.746455\n",
      "train epoch 180: loss_ae: 0.021076 loss_mask_ae: 0.051391 loss_corr: 0.742900\n",
      "train epoch 190: loss_ae: 0.020966 loss_mask_ae: 0.051312 loss_corr: 0.738013\n",
      "train epoch 200: loss_ae: 0.020854 loss_mask_ae: 0.052623 loss_corr: 0.733565\n",
      "train epoch 210: loss_ae: 0.020797 loss_mask_ae: 0.053078 loss_corr: 0.730473\n",
      "train epoch 220: loss_ae: 0.020763 loss_mask_ae: 0.051082 loss_corr: 0.727940\n",
      "train epoch 230: loss_ae: 0.020688 loss_mask_ae: 0.051826 loss_corr: 0.725084\n",
      "train epoch 240: loss_ae: 0.020664 loss_mask_ae: 0.051067 loss_corr: 0.723247\n",
      "train epoch 250: loss_ae: 0.020587 loss_mask_ae: 0.051226 loss_corr: 0.719827\n",
      "train epoch 260: loss_ae: 0.020823 loss_mask_ae: 0.050919 loss_corr: 0.731383\n",
      "train epoch 270: loss_ae: 0.020577 loss_mask_ae: 0.051779 loss_corr: 0.719158\n",
      "train epoch 280: loss_ae: 0.020497 loss_mask_ae: 0.051607 loss_corr: 0.715397\n",
      "train epoch 290: loss_ae: 0.020440 loss_mask_ae: 0.050817 loss_corr: 0.712670\n",
      "train epoch 300: loss_ae: 0.020391 loss_mask_ae: 0.051032 loss_corr: 0.710480\n",
      "train epoch 310: loss_ae: 0.020347 loss_mask_ae: 0.051250 loss_corr: 0.708569\n",
      "train epoch 320: loss_ae: 0.020314 loss_mask_ae: 0.050037 loss_corr: 0.706736\n",
      "train epoch 330: loss_ae: 0.020280 loss_mask_ae: 0.051270 loss_corr: 0.705118\n",
      "train epoch 340: loss_ae: 0.020216 loss_mask_ae: 0.051261 loss_corr: 0.702016\n",
      "train epoch 350: loss_ae: 0.020217 loss_mask_ae: 0.050768 loss_corr: 0.702396\n",
      "train epoch 360: loss_ae: 0.020185 loss_mask_ae: 0.050617 loss_corr: 0.700383\n",
      "train epoch 370: loss_ae: 0.020097 loss_mask_ae: 0.050500 loss_corr: 0.696196\n",
      "train epoch 380: loss_ae: 0.020158 loss_mask_ae: 0.049770 loss_corr: 0.698634\n",
      "train epoch 390: loss_ae: 0.020118 loss_mask_ae: 0.052190 loss_corr: 0.694981\n",
      "Finished 5 Times Training and Testing...\n",
      "(1549, 2) (1549, 996) (15413, 996)\n",
      "333.0 0.0 411165.0 0.0\n",
      "Before Norm ST: 0.7857480605677198 Before Norm RNA: 0.49393210107062613\n",
      "Xrna shape: (15413, 897) Xst shape: (1549, 897)\n",
      "ST: 0.7857487802754033 RNA: 0.49393210107062613\n",
      "1.0 0.0 0.05334712 1.0 0.0 0.037494946\n",
      "train/STARmap_6.pkl\n",
      "{'dims': [2222, 512, 256], 'pretrain_epochs': 400, 'epochs': 15, 'pre_lr': 0.0001, 'lr': 0.0001, 'batch_size': 256, 'weight_map': 10, 'weight_coef': 0, 'weight_mmd': 0, 'weight_ent': 0, 'alpha': 1, 'ot': {'epochs': 300, 'lr': 0.05, 'step_size': 300, 'tau': 1, 'it': 3, 'epsilon': 1, 'num_iter': 5}, 'num_sample1': 1549, 'num_sample2': 15413}\n",
      "\n",
      "===========> Training... <===========\n",
      "train epoch 0: loss_ae: 0.028824 loss_mask_ae: 0.086422 loss_corr: 0.994713\n",
      "train epoch 10: loss_ae: 0.023565 loss_mask_ae: 0.064440 loss_corr: 0.801133\n",
      "train epoch 20: loss_ae: 0.022578 loss_mask_ae: 0.055089 loss_corr: 0.769829\n",
      "train epoch 30: loss_ae: 0.022417 loss_mask_ae: 0.057051 loss_corr: 0.763723\n",
      "train epoch 40: loss_ae: 0.022371 loss_mask_ae: 0.056083 loss_corr: 0.762078\n",
      "train epoch 50: loss_ae: 0.022341 loss_mask_ae: 0.055939 loss_corr: 0.761224\n",
      "train epoch 60: loss_ae: 0.022313 loss_mask_ae: 0.056024 loss_corr: 0.760736\n",
      "train epoch 70: loss_ae: 0.022277 loss_mask_ae: 0.056137 loss_corr: 0.759925\n",
      "train epoch 80: loss_ae: 0.022196 loss_mask_ae: 0.055634 loss_corr: 0.758434\n",
      "train epoch 90: loss_ae: 0.022100 loss_mask_ae: 0.054693 loss_corr: 0.756944\n",
      "train epoch 100: loss_ae: 0.022023 loss_mask_ae: 0.055382 loss_corr: 0.754839\n",
      "train epoch 110: loss_ae: 0.021926 loss_mask_ae: 0.055638 loss_corr: 0.753717\n",
      "train epoch 120: loss_ae: 0.021756 loss_mask_ae: 0.054763 loss_corr: 0.753181\n",
      "train epoch 130: loss_ae: 0.021652 loss_mask_ae: 0.052324 loss_corr: 0.751466\n",
      "train epoch 140: loss_ae: 0.021503 loss_mask_ae: 0.052731 loss_corr: 0.748604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 150: loss_ae: 0.021465 loss_mask_ae: 0.055888 loss_corr: 0.746043\n",
      "train epoch 160: loss_ae: 0.021378 loss_mask_ae: 0.054059 loss_corr: 0.742635\n",
      "train epoch 170: loss_ae: 0.021284 loss_mask_ae: 0.050722 loss_corr: 0.739193\n",
      "train epoch 180: loss_ae: 0.021134 loss_mask_ae: 0.052846 loss_corr: 0.735703\n",
      "train epoch 190: loss_ae: 0.021043 loss_mask_ae: 0.052563 loss_corr: 0.732632\n",
      "train epoch 200: loss_ae: 0.020971 loss_mask_ae: 0.051355 loss_corr: 0.729913\n",
      "train epoch 210: loss_ae: 0.020894 loss_mask_ae: 0.052506 loss_corr: 0.726907\n",
      "train epoch 220: loss_ae: 0.020912 loss_mask_ae: 0.052345 loss_corr: 0.727751\n",
      "train epoch 230: loss_ae: 0.020842 loss_mask_ae: 0.051215 loss_corr: 0.724574\n",
      "train epoch 240: loss_ae: 0.020770 loss_mask_ae: 0.051959 loss_corr: 0.721265\n",
      "train epoch 250: loss_ae: 0.020722 loss_mask_ae: 0.051682 loss_corr: 0.718879\n",
      "train epoch 260: loss_ae: 0.020685 loss_mask_ae: 0.051166 loss_corr: 0.716794\n",
      "train epoch 270: loss_ae: 0.020659 loss_mask_ae: 0.051632 loss_corr: 0.715162\n",
      "train epoch 280: loss_ae: 0.020607 loss_mask_ae: 0.050963 loss_corr: 0.712366\n",
      "train epoch 290: loss_ae: 0.020571 loss_mask_ae: 0.050745 loss_corr: 0.710217\n",
      "train epoch 300: loss_ae: 0.020556 loss_mask_ae: 0.051782 loss_corr: 0.709083\n",
      "train epoch 310: loss_ae: 0.020588 loss_mask_ae: 0.048871 loss_corr: 0.708437\n",
      "train epoch 320: loss_ae: 0.020487 loss_mask_ae: 0.051268 loss_corr: 0.705209\n",
      "train epoch 330: loss_ae: 0.020410 loss_mask_ae: 0.050962 loss_corr: 0.701757\n",
      "train epoch 340: loss_ae: 0.020364 loss_mask_ae: 0.050702 loss_corr: 0.699397\n",
      "train epoch 350: loss_ae: 0.020418 loss_mask_ae: 0.049079 loss_corr: 0.700095\n",
      "train epoch 360: loss_ae: 0.020344 loss_mask_ae: 0.051779 loss_corr: 0.697403\n",
      "train epoch 370: loss_ae: 0.020247 loss_mask_ae: 0.049713 loss_corr: 0.693138\n",
      "train epoch 380: loss_ae: 0.020218 loss_mask_ae: 0.049512 loss_corr: 0.691134\n",
      "train epoch 390: loss_ae: 0.020228 loss_mask_ae: 0.051856 loss_corr: 0.690507\n",
      "Finished 6 Times Training and Testing...\n",
      "(1549, 2) (1549, 996) (15413, 996)\n",
      "333.0 0.0 207493.0 0.0\n",
      "Before Norm ST: 0.7900641475458328 Before Norm RNA: 0.5158210637605501\n",
      "Xrna shape: (15413, 897) Xst shape: (1549, 897)\n",
      "ST: 0.7900648672535163 RNA: 0.5158210637605501\n",
      "1.0 0.0 0.05254667 1.0 0.0 0.036043543\n",
      "train/STARmap_7.pkl\n",
      "{'dims': [2222, 512, 256], 'pretrain_epochs': 400, 'epochs': 15, 'pre_lr': 0.0001, 'lr': 0.0001, 'batch_size': 256, 'weight_map': 10, 'weight_coef': 0, 'weight_mmd': 0, 'weight_ent': 0, 'alpha': 1, 'ot': {'epochs': 300, 'lr': 0.05, 'step_size': 300, 'tau': 1, 'it': 3, 'epsilon': 1, 'num_iter': 5}, 'num_sample1': 1549, 'num_sample2': 15413}\n",
      "\n",
      "===========> Training... <===========\n",
      "train epoch 0: loss_ae: 0.028688 loss_mask_ae: 0.087238 loss_corr: 1.001740\n",
      "train epoch 10: loss_ae: 0.023542 loss_mask_ae: 0.065781 loss_corr: 0.806024\n",
      "train epoch 20: loss_ae: 0.022429 loss_mask_ae: 0.055869 loss_corr: 0.771067\n",
      "train epoch 30: loss_ae: 0.022290 loss_mask_ae: 0.057912 loss_corr: 0.765749\n",
      "train epoch 40: loss_ae: 0.022236 loss_mask_ae: 0.057235 loss_corr: 0.763776\n",
      "train epoch 50: loss_ae: 0.022201 loss_mask_ae: 0.056993 loss_corr: 0.762859\n",
      "train epoch 60: loss_ae: 0.022165 loss_mask_ae: 0.057095 loss_corr: 0.761857\n",
      "train epoch 70: loss_ae: 0.022107 loss_mask_ae: 0.056671 loss_corr: 0.760086\n",
      "train epoch 80: loss_ae: 0.022009 loss_mask_ae: 0.056117 loss_corr: 0.757922\n",
      "train epoch 90: loss_ae: 0.021919 loss_mask_ae: 0.054823 loss_corr: 0.756512\n",
      "train epoch 100: loss_ae: 0.021837 loss_mask_ae: 0.054639 loss_corr: 0.755212\n",
      "train epoch 110: loss_ae: 0.021696 loss_mask_ae: 0.055336 loss_corr: 0.755006\n",
      "train epoch 120: loss_ae: 0.021610 loss_mask_ae: 0.055417 loss_corr: 0.754495\n",
      "train epoch 130: loss_ae: 0.021542 loss_mask_ae: 0.056088 loss_corr: 0.753690\n",
      "train epoch 140: loss_ae: 0.021409 loss_mask_ae: 0.055523 loss_corr: 0.750387\n",
      "train epoch 150: loss_ae: 0.021251 loss_mask_ae: 0.053865 loss_corr: 0.745400\n",
      "train epoch 160: loss_ae: 0.021119 loss_mask_ae: 0.052709 loss_corr: 0.740683\n",
      "train epoch 170: loss_ae: 0.021294 loss_mask_ae: 0.048788 loss_corr: 0.738448\n",
      "train epoch 180: loss_ae: 0.021208 loss_mask_ae: 0.050892 loss_corr: 0.740101\n",
      "train epoch 190: loss_ae: 0.020970 loss_mask_ae: 0.052582 loss_corr: 0.734816\n",
      "train epoch 200: loss_ae: 0.020857 loss_mask_ae: 0.053142 loss_corr: 0.730977\n",
      "train epoch 210: loss_ae: 0.020829 loss_mask_ae: 0.052978 loss_corr: 0.730030\n",
      "train epoch 220: loss_ae: 0.020743 loss_mask_ae: 0.052157 loss_corr: 0.726440\n",
      "train epoch 230: loss_ae: 0.020708 loss_mask_ae: 0.052343 loss_corr: 0.724679\n",
      "train epoch 240: loss_ae: 0.020652 loss_mask_ae: 0.052225 loss_corr: 0.722281\n",
      "train epoch 250: loss_ae: 0.020608 loss_mask_ae: 0.052097 loss_corr: 0.720192\n",
      "train epoch 260: loss_ae: 0.020572 loss_mask_ae: 0.051981 loss_corr: 0.718320\n",
      "train epoch 270: loss_ae: 0.020529 loss_mask_ae: 0.051767 loss_corr: 0.716152\n",
      "train epoch 280: loss_ae: 0.020528 loss_mask_ae: 0.051574 loss_corr: 0.715940\n",
      "train epoch 290: loss_ae: 0.020462 loss_mask_ae: 0.051210 loss_corr: 0.712362\n",
      "train epoch 300: loss_ae: 0.020411 loss_mask_ae: 0.051846 loss_corr: 0.709696\n",
      "train epoch 310: loss_ae: 0.020381 loss_mask_ae: 0.051407 loss_corr: 0.708012\n",
      "train epoch 320: loss_ae: 0.020330 loss_mask_ae: 0.051551 loss_corr: 0.705334\n",
      "train epoch 330: loss_ae: 0.020295 loss_mask_ae: 0.051144 loss_corr: 0.703363\n",
      "train epoch 340: loss_ae: 0.020271 loss_mask_ae: 0.050811 loss_corr: 0.702036\n",
      "train epoch 350: loss_ae: 0.020247 loss_mask_ae: 0.051031 loss_corr: 0.700359\n",
      "train epoch 360: loss_ae: 0.020201 loss_mask_ae: 0.051490 loss_corr: 0.697907\n",
      "train epoch 370: loss_ae: 0.020146 loss_mask_ae: 0.051015 loss_corr: 0.695109\n",
      "train epoch 380: loss_ae: 0.020150 loss_mask_ae: 0.050163 loss_corr: 0.694839\n",
      "train epoch 390: loss_ae: 0.020076 loss_mask_ae: 0.050754 loss_corr: 0.691125\n",
      "Finished 7 Times Training and Testing...\n",
      "(1549, 2) (1549, 996) (15413, 996)\n",
      "333.0 0.0 411165.0 0.0\n",
      "Before Norm ST: 0.7885102986570974 Before Norm RNA: 0.5087901228031384\n",
      "Xrna shape: (15413, 897) Xst shape: (1549, 897)\n",
      "ST: 0.788511018364781 RNA: 0.5087901228031384\n",
      "1.0 0.0 0.05299163 1.0 0.0 0.0364938\n",
      "train/STARmap_8.pkl\n",
      "{'dims': [2222, 512, 256], 'pretrain_epochs': 400, 'epochs': 15, 'pre_lr': 0.0001, 'lr': 0.0001, 'batch_size': 256, 'weight_map': 10, 'weight_coef': 0, 'weight_mmd': 0, 'weight_ent': 0, 'alpha': 1, 'ot': {'epochs': 300, 'lr': 0.05, 'step_size': 300, 'tau': 1, 'it': 3, 'epsilon': 1, 'num_iter': 5}, 'num_sample1': 1549, 'num_sample2': 15413}\n",
      "\n",
      "===========> Training... <===========\n",
      "train epoch 0: loss_ae: 0.028894 loss_mask_ae: 0.087237 loss_corr: 1.007908\n",
      "train epoch 10: loss_ae: 0.023622 loss_mask_ae: 0.065661 loss_corr: 0.804086\n",
      "train epoch 20: loss_ae: 0.022540 loss_mask_ae: 0.055791 loss_corr: 0.769816\n",
      "train epoch 30: loss_ae: 0.022392 loss_mask_ae: 0.057766 loss_corr: 0.764312\n",
      "train epoch 40: loss_ae: 0.022343 loss_mask_ae: 0.056838 loss_corr: 0.762580\n",
      "train epoch 50: loss_ae: 0.022314 loss_mask_ae: 0.056640 loss_corr: 0.761766\n",
      "train epoch 60: loss_ae: 0.022286 loss_mask_ae: 0.056796 loss_corr: 0.761221\n",
      "train epoch 70: loss_ae: 0.022248 loss_mask_ae: 0.056700 loss_corr: 0.760408\n",
      "train epoch 80: loss_ae: 0.022161 loss_mask_ae: 0.056691 loss_corr: 0.758970\n",
      "train epoch 90: loss_ae: 0.022108 loss_mask_ae: 0.053590 loss_corr: 0.758223\n",
      "train epoch 100: loss_ae: 0.022023 loss_mask_ae: 0.057994 loss_corr: 0.757027\n",
      "train epoch 110: loss_ae: 0.021834 loss_mask_ae: 0.053474 loss_corr: 0.755080\n",
      "train epoch 120: loss_ae: 0.021685 loss_mask_ae: 0.054099 loss_corr: 0.754175\n",
      "train epoch 130: loss_ae: 0.021729 loss_mask_ae: 0.058081 loss_corr: 0.753834\n",
      "train epoch 140: loss_ae: 0.021522 loss_mask_ae: 0.053250 loss_corr: 0.750983\n",
      "train epoch 150: loss_ae: 0.021410 loss_mask_ae: 0.055234 loss_corr: 0.747539\n",
      "train epoch 160: loss_ae: 0.021257 loss_mask_ae: 0.053770 loss_corr: 0.742684\n",
      "train epoch 170: loss_ae: 0.021123 loss_mask_ae: 0.052309 loss_corr: 0.737772\n",
      "train epoch 180: loss_ae: 0.021041 loss_mask_ae: 0.052058 loss_corr: 0.734856\n",
      "train epoch 190: loss_ae: 0.021030 loss_mask_ae: 0.055292 loss_corr: 0.732375\n",
      "train epoch 200: loss_ae: 0.020948 loss_mask_ae: 0.053612 loss_corr: 0.730586\n",
      "train epoch 210: loss_ae: 0.020926 loss_mask_ae: 0.052849 loss_corr: 0.730520\n",
      "train epoch 220: loss_ae: 0.020837 loss_mask_ae: 0.051962 loss_corr: 0.726167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 230: loss_ae: 0.020781 loss_mask_ae: 0.052796 loss_corr: 0.723584\n",
      "train epoch 240: loss_ae: 0.020740 loss_mask_ae: 0.051058 loss_corr: 0.721367\n",
      "train epoch 250: loss_ae: 0.020695 loss_mask_ae: 0.052655 loss_corr: 0.719085\n",
      "train epoch 260: loss_ae: 0.020663 loss_mask_ae: 0.052038 loss_corr: 0.717378\n",
      "train epoch 270: loss_ae: 0.020629 loss_mask_ae: 0.051944 loss_corr: 0.715525\n",
      "train epoch 280: loss_ae: 0.020680 loss_mask_ae: 0.050941 loss_corr: 0.716490\n",
      "train epoch 290: loss_ae: 0.020577 loss_mask_ae: 0.051533 loss_corr: 0.712199\n",
      "train epoch 300: loss_ae: 0.020522 loss_mask_ae: 0.050471 loss_corr: 0.708885\n",
      "train epoch 310: loss_ae: 0.020482 loss_mask_ae: 0.051104 loss_corr: 0.706959\n",
      "train epoch 320: loss_ae: 0.020493 loss_mask_ae: 0.052562 loss_corr: 0.706507\n",
      "train epoch 330: loss_ae: 0.020418 loss_mask_ae: 0.050694 loss_corr: 0.702893\n",
      "train epoch 340: loss_ae: 0.020362 loss_mask_ae: 0.051199 loss_corr: 0.700556\n",
      "train epoch 350: loss_ae: 0.020344 loss_mask_ae: 0.050640 loss_corr: 0.699623\n",
      "train epoch 360: loss_ae: 0.020310 loss_mask_ae: 0.050554 loss_corr: 0.697712\n",
      "train epoch 370: loss_ae: 0.020244 loss_mask_ae: 0.050732 loss_corr: 0.694598\n",
      "train epoch 380: loss_ae: 0.020214 loss_mask_ae: 0.051133 loss_corr: 0.693103\n",
      "train epoch 390: loss_ae: 0.020175 loss_mask_ae: 0.050295 loss_corr: 0.690985\n",
      "Finished 8 Times Training and Testing...\n",
      "(1549, 2) (1549, 996) (15413, 996)\n",
      "165.0 0.0 411165.0 0.0\n",
      "Before Norm ST: 0.7874926319925899 Before Norm RNA: 0.5176364100987302\n",
      "Xrna shape: (15413, 897) Xst shape: (1549, 897)\n",
      "ST: 0.7874926319925899 RNA: 0.5176364100987302\n",
      "1.0 0.0 0.052879885 1.0 0.0 0.035847917\n",
      "train/STARmap_9.pkl\n",
      "{'dims': [2222, 512, 256], 'pretrain_epochs': 400, 'epochs': 15, 'pre_lr': 0.0001, 'lr': 0.0001, 'batch_size': 256, 'weight_map': 10, 'weight_coef': 0, 'weight_mmd': 0, 'weight_ent': 0, 'alpha': 1, 'ot': {'epochs': 300, 'lr': 0.05, 'step_size': 300, 'tau': 1, 'it': 3, 'epsilon': 1, 'num_iter': 5}, 'num_sample1': 1549, 'num_sample2': 15413}\n",
      "\n",
      "===========> Training... <===========\n",
      "train epoch 0: loss_ae: 0.028728 loss_mask_ae: 0.086150 loss_corr: 1.004327\n",
      "train epoch 10: loss_ae: 0.023467 loss_mask_ae: 0.062266 loss_corr: 0.807523\n",
      "train epoch 20: loss_ae: 0.022437 loss_mask_ae: 0.055852 loss_corr: 0.771314\n",
      "train epoch 30: loss_ae: 0.022294 loss_mask_ae: 0.056754 loss_corr: 0.765761\n",
      "train epoch 40: loss_ae: 0.022245 loss_mask_ae: 0.056092 loss_corr: 0.763986\n",
      "train epoch 50: loss_ae: 0.022212 loss_mask_ae: 0.055995 loss_corr: 0.763195\n",
      "train epoch 60: loss_ae: 0.022184 loss_mask_ae: 0.056227 loss_corr: 0.762669\n",
      "train epoch 70: loss_ae: 0.022140 loss_mask_ae: 0.055824 loss_corr: 0.761777\n",
      "train epoch 80: loss_ae: 0.022042 loss_mask_ae: 0.055384 loss_corr: 0.760265\n",
      "train epoch 90: loss_ae: 0.022009 loss_mask_ae: 0.052751 loss_corr: 0.759938\n",
      "train epoch 100: loss_ae: 0.021829 loss_mask_ae: 0.053802 loss_corr: 0.756799\n",
      "train epoch 110: loss_ae: 0.021729 loss_mask_ae: 0.055726 loss_corr: 0.755879\n",
      "train epoch 120: loss_ae: 0.021605 loss_mask_ae: 0.055523 loss_corr: 0.754503\n",
      "train epoch 130: loss_ae: 0.021472 loss_mask_ae: 0.054554 loss_corr: 0.752419\n",
      "train epoch 140: loss_ae: 0.021351 loss_mask_ae: 0.052996 loss_corr: 0.749035\n",
      "train epoch 150: loss_ae: 0.021220 loss_mask_ae: 0.052203 loss_corr: 0.744169\n",
      "train epoch 160: loss_ae: 0.021044 loss_mask_ae: 0.052327 loss_corr: 0.738410\n",
      "train epoch 170: loss_ae: 0.020980 loss_mask_ae: 0.050655 loss_corr: 0.735731\n",
      "train epoch 180: loss_ae: 0.020867 loss_mask_ae: 0.051069 loss_corr: 0.732244\n",
      "train epoch 190: loss_ae: 0.020797 loss_mask_ae: 0.051114 loss_corr: 0.729418\n",
      "train epoch 200: loss_ae: 0.021012 loss_mask_ae: 0.055925 loss_corr: 0.731345\n",
      "train epoch 210: loss_ae: 0.020768 loss_mask_ae: 0.050608 loss_corr: 0.727861\n",
      "train epoch 220: loss_ae: 0.020683 loss_mask_ae: 0.052241 loss_corr: 0.724569\n",
      "train epoch 230: loss_ae: 0.020641 loss_mask_ae: 0.051367 loss_corr: 0.722498\n",
      "train epoch 240: loss_ae: 0.020596 loss_mask_ae: 0.051015 loss_corr: 0.720259\n",
      "train epoch 250: loss_ae: 0.020550 loss_mask_ae: 0.051185 loss_corr: 0.717715\n",
      "train epoch 260: loss_ae: 0.020526 loss_mask_ae: 0.050962 loss_corr: 0.716262\n",
      "train epoch 270: loss_ae: 0.020485 loss_mask_ae: 0.051459 loss_corr: 0.713716\n",
      "train epoch 280: loss_ae: 0.020424 loss_mask_ae: 0.051285 loss_corr: 0.710541\n",
      "train epoch 290: loss_ae: 0.020413 loss_mask_ae: 0.050130 loss_corr: 0.709443\n",
      "train epoch 300: loss_ae: 0.020366 loss_mask_ae: 0.051683 loss_corr: 0.706822\n",
      "train epoch 310: loss_ae: 0.020330 loss_mask_ae: 0.051931 loss_corr: 0.704507\n",
      "train epoch 320: loss_ae: 0.020317 loss_mask_ae: 0.049386 loss_corr: 0.703765\n",
      "train epoch 330: loss_ae: 0.020208 loss_mask_ae: 0.049875 loss_corr: 0.698997\n",
      "train epoch 340: loss_ae: 0.020159 loss_mask_ae: 0.049814 loss_corr: 0.696713\n",
      "train epoch 350: loss_ae: 0.020180 loss_mask_ae: 0.048192 loss_corr: 0.695956\n",
      "train epoch 360: loss_ae: 0.020078 loss_mask_ae: 0.050422 loss_corr: 0.692445\n",
      "train epoch 370: loss_ae: 0.020084 loss_mask_ae: 0.048413 loss_corr: 0.691279\n",
      "train epoch 380: loss_ae: 0.020043 loss_mask_ae: 0.050960 loss_corr: 0.690118\n",
      "train epoch 390: loss_ae: 0.019988 loss_mask_ae: 0.049121 loss_corr: 0.687551\n",
      "Finished 9 Times Training and Testing...\n"
     ]
    }
   ],
   "source": [
    "for train_idx, test_idx in cv.split(atlas_genes):\n",
    "    x1_train_cell = x1_cell[:, train_idx]\n",
    "    x1_test_cell = x1_cell[:, test_idx]\n",
    "    x2_train_cell = x2_rna[:, train_idx]\n",
    "    x2_test_cell = x2_rna[:, test_idx]\n",
    "    print(x1_loc.shape, x1_cell.shape, x2_rna.shape)\n",
    "    x1 = x1_train_cell.astype(np.float32)\n",
    "    x2 = x2_train_cell.astype(np.float32)\n",
    "    print (x1.max(), x1.min(), x2.max(), x2.min())\n",
    "    count_zerox1 = np.sum(x1 == 0)\n",
    "    count_zerox2 = np.sum(x2 == 0)\n",
    "    print('Before Norm ST:', float(count_zerox1) / float(x1.shape[0] * x1.shape[1]), 'Before Norm RNA:',\n",
    "          float(count_zerox2) / float(x2.shape[0] * x2.shape[1]))\n",
    "    x1, x2 = normalize_type(x1, x2, type='all')\n",
    "    print ('Xrna shape:', x2.shape,  'Xst shape:', x1.shape)\n",
    "    count_zerox1 = np.sum(x1 == 0)\n",
    "    count_zerox2 = np.sum(x2 == 0)\n",
    "    print ('ST:', float(count_zerox1) / float(x1.shape[0]*x1.shape[1]), 'RNA:', float(count_zerox2) / float(x2.shape[0]*x2.shape[1]))\n",
    "    print (x1.max(), x1.min(), x1.mean(), x2.max(), x2.min(), x2.mean())\n",
    "    config = get_default_config(data_name)\n",
    "    config['num_sample1'] = x1.shape[0]\n",
    "    config['num_sample2'] = x2.shape[0]\n",
    "    train_path = \"train/%s_%d.pkl\" % (data_name, cnts)\n",
    "    print (train_path)\n",
    "    tmp_dims = x1.shape[1]\n",
    "    model = Model(config)\n",
    "    model.to(device)\n",
    "    optimizer_pre = torch.optim.Adam(model.parameters(), lr=config['pre_lr'])\n",
    "    train(model, optimizer_pre, x1, x2, config, train_path)\n",
    "    print(\"Finished \" + str(cnts) + ' Times Training and Testing...')\n",
    "    cnts += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.10",
   "language": "python",
   "name": "torch1.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
